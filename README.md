# Contextual Safety Benchmark for LLMs

This repository contains code, data, and analysis for evaluating the safety behavior of large language models (LLMs) under contextual and multilingual variations of harmful prompts.

## Repository Structure

### `Multilingual/`
Contains all code related to our multilingual perturbation pipeline. This includes:
- Full translation of prompts into nine low-resource and high-resource languages
- Code-switching (partial translation at the token level)
- Evaluation utilities for COMET and BERTScore

### `data/`
Includes all datasets used in this project:
- `intersection.csv`: A curated set of 45 harmful prompts that remain harmful across all contexts. This is ideal for benchmarking harmfulness consistency across contexts and models.
- `safe.csv`: Contains our manually verified safe prompt dataset, including:
  - `Safe1`: Topic-preserving rewrites generated by an LLM and verified for safety
  - `Safe2`: Structure-preserving rewrites created through minimal human edits
- Additional CSV files contain the full set of synthetic prompt-context pairs generated by LLMs, covering Original, Cultural, Research, Satirical, and Imaginary variations.

### `harm_rating/`
Code for running the harmfulness evaluation pipeline using WildGuard. This includes scoring model outputs on both instructions and responses.

### `research/`
All code related to our layer-wise and latent space analyses of LLM representations. This includes:
- Cosine similarity computation across transformer layers
- PCA and silhouette clustering to analyze contextual embedding separation

> **Note:** Some scripts have placeholder variables for paths, model names, or API keys. Before running, please double-check the top of each script and insert the correct file paths, model directories, and tokens where applicable.

## Citation and Use
If you use this code or dataset in your work, please cite our paper (coming soon). This project was developed for research purposes and supports further analysis on the robustness and safety of LLMs under indirect prompt variations.

---

For questions or feedback, feel free to open an issue or contact us directly.
